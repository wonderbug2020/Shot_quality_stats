{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91f72834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ModelMaker as mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9663925",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_11 = pd.read_csv('data/pbp/ready_to_model/version_4/10_11_pbp_cleaned.csv')\n",
    "df_12 = pd.read_csv('data/pbp/ready_to_model/version_4/11_12_pbp_cleaned.csv')\n",
    "df_13 = pd.read_csv('data/pbp/ready_to_model/version_4/12_13_pbp_cleaned.csv')\n",
    "df_14 = pd.read_csv('data/pbp/ready_to_model/version_4/13_14_pbp_cleaned.csv')\n",
    "df_15 = pd.read_csv('data/pbp/ready_to_model/version_4/14_15_pbp_cleaned.csv')\n",
    "df_16 = pd.read_csv('data/pbp/ready_to_model/version_4/15_16_pbp_cleaned.csv')\n",
    "df_17 = pd.read_csv('data/pbp/ready_to_model/version_4/16_17_pbp_cleaned.csv')\n",
    "df_18 = pd.read_csv('data/pbp/ready_to_model/version_4/17_18_pbp_cleaned.csv')\n",
    "df_19 = pd.read_csv('data/pbp/ready_to_model/version_4/18_19_pbp_cleaned.csv')\n",
    "df_20 = pd.read_csv('data/pbp/ready_to_model/version_4/19_20_pbp_cleaned.csv')\n",
    "df_21 = pd.read_csv('data/pbp/ready_to_model/version_4/20_21_pbp_cleaned.csv')\n",
    "df = pd.concat([df_11,df_12,df_13,df_14,df_15,df_16,df_17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b47dd04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1142a40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19077\n"
     ]
    }
   ],
   "source": [
    "mm.run_XGB(df_1421)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm.run_tabnet(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c727699",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGB baseline = .18822\n",
    "#XGB, only 2021            ll = .19647\n",
    "#XGB, season 20 through 21 ll = .19994\n",
    "#XGB, season 19 through 21 ll = .19602\n",
    "#XGB, season 18 through 21 ll = .19547"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85f6b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c7fdbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "171ccdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for year 21\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19647\n",
      "for year 20\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19994\n",
      "for year 19\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19602\n",
      "for year 18\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19547\n",
      "for year 17\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19546\n",
      "for year 16\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19521\n",
      "for year 15\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19258\n",
      "for year 14\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19077\n",
      "for year 13\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.18878\n",
      "for year 12\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.1881\n",
      "for year 11\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.1899\n"
     ]
    }
   ],
   "source": [
    "df_2021 = pd.concat([df_21,df_20])\n",
    "df_1921 = pd.concat([df_21,df_20,df_19])\n",
    "df_1821 = pd.concat([df_21,df_20,df_19,df_18])\n",
    "df_1721 = pd.concat([df_21,df_20,df_19,df_18,df_17])\n",
    "df_1621 = pd.concat([df_21,df_20,df_19,df_18,df_17,df_16])\n",
    "df_1521 = pd.concat([df_21,df_20,df_19,df_18,df_17,df_16,df_15])\n",
    "df_1421 = pd.concat([df_21,df_20,df_19,df_18,df_17,df_16,df_15,df_14])\n",
    "df_1321 = pd.concat([df_21,df_20,df_19,df_18,df_17,df_16,df_15,df_14,df_13])\n",
    "df_1221 = pd.concat([df_21,df_20,df_19,df_18,df_17,df_16,df_15,df_14,df_13,df_12])\n",
    "df_1121 = pd.concat([df_21,df_20,df_19,df_18,df_17,df_16,df_15,df_14,df_13,df_12,df_11])\n",
    "lst = [df_21,df_2021,df_1921,df_1821,df_1721,df_1621,df_1521,df_1421,df_1321,df_1221,df_1121]\n",
    "start_year = 21\n",
    "for df in lst:\n",
    "    print(f'for year {start_year}')\n",
    "    mm.run_XGB(df)    \n",
    "    start_year -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bb4be63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for year 20\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.199\n",
      "for year 19\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19702\n",
      "for year 18\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19302\n",
      "for year 17\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19377\n",
      "for year 16\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.18889\n",
      "for year 15\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.1907\n",
      "for year 14\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19019\n",
      "for year 13\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.1902\n",
      "for year 12\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19191\n",
      "for year 11\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.18913\n"
     ]
    }
   ],
   "source": [
    "df_2020 = pd.concat([df_20])\n",
    "df_1920 = pd.concat([df_20,df_19])\n",
    "df_1820 = pd.concat([df_20,df_19,df_18])\n",
    "df_1720 = pd.concat([df_20,df_19,df_18,df_17])\n",
    "df_1620 = pd.concat([df_20,df_19,df_18,df_17,df_16])\n",
    "df_1520 = pd.concat([df_20,df_19,df_18,df_17,df_16,df_15])\n",
    "df_1420 = pd.concat([df_20,df_19,df_18,df_17,df_16,df_15,df_14])\n",
    "df_1320 = pd.concat([df_20,df_19,df_18,df_17,df_16,df_15,df_14,df_13])\n",
    "df_1220 = pd.concat([df_20,df_19,df_18,df_17,df_16,df_15,df_14,df_13,df_12])\n",
    "df_1120 = pd.concat([df_20,df_19,df_18,df_17,df_16,df_15,df_14,df_13,df_12,df_11])\n",
    "lst = [df_2020,df_1920,df_1820,df_1720,df_1620,df_1520,df_1420,df_1320,df_1220,df_1120]\n",
    "start_year = 20\n",
    "for df in lst:\n",
    "    print(f'for year {start_year}')\n",
    "    mm.run_XGB(df)    \n",
    "    start_year -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b572da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for year 13\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.18909\n",
      "for year 12\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19048\n",
      "for year 11\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19164\n"
     ]
    }
   ],
   "source": [
    "df_1320_14 = pd.concat([df_20,df_19,df_18,df_17,df_16,df_15,df_13])\n",
    "df_1220_14 = pd.concat([df_20,df_19,df_18,df_17,df_16,df_15,df_13,df_12])\n",
    "df_1120_14 = pd.concat([df_20,df_19,df_18,df_17,df_16,df_15,df_13,df_12,df_11])\n",
    "lst = [df_1320_14,df_1220_14,df_1120_14]\n",
    "start_year = 13\n",
    "for df in lst:\n",
    "    print(f'for year {start_year}')\n",
    "    mm.run_XGB(df)    \n",
    "    start_year -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3944a801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for year 14\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19144\n",
      "for year 13\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.18938\n",
      "for year 12\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19098\n",
      "for year 11\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19063\n"
     ]
    }
   ],
   "source": [
    "df_1420_15 = pd.concat([df_20,df_19,df_18,df_17,df_16,df_14])\n",
    "df_1320_15 = pd.concat([df_20,df_19,df_18,df_17,df_16,df_14,df_13])\n",
    "df_1220_15 = pd.concat([df_20,df_19,df_18,df_17,df_16,df_14,df_13,df_12])\n",
    "df_1120_15 = pd.concat([df_20,df_19,df_18,df_17,df_16,df_14,df_13,df_12,df_11])\n",
    "lst = [df_1420_15,df_1320_15,df_1220_15,df_1120_15]\n",
    "start_year = 14\n",
    "for df in lst:\n",
    "    print(f'for year {start_year}')\n",
    "    mm.run_XGB(df)    \n",
    "    start_year -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ffadee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for year 20\n",
      "Device used : cpu\n",
      "Running the TabNet DNN, this could take a while\n",
      "epoch 0  | loss: 0.12765 | train_logloss: 0.28953 | val_logloss: 0.28001 |  0:00:04s\n",
      "epoch 1  | loss: 0.05734 | train_logloss: 0.22913 | val_logloss: 0.23885 |  0:00:09s\n",
      "epoch 2  | loss: 0.05677 | train_logloss: 0.22997 | val_logloss: 0.23475 |  0:00:14s\n",
      "epoch 3  | loss: 0.05629 | train_logloss: 0.2214  | val_logloss: 0.22514 |  0:00:19s\n",
      "epoch 4  | loss: 0.05601 | train_logloss: 0.22459 | val_logloss: 0.22449 |  0:00:23s\n",
      "epoch 5  | loss: 0.05571 | train_logloss: 0.22144 | val_logloss: 0.24    |  0:00:28s\n",
      "epoch 6  | loss: 0.05541 | train_logloss: 0.22023 | val_logloss: 0.22986 |  0:00:33s\n",
      "epoch 7  | loss: 0.05528 | train_logloss: 0.21696 | val_logloss: 0.22733 |  0:00:38s\n",
      "epoch 8  | loss: 0.05526 | train_logloss: 0.21642 | val_logloss: 0.23234 |  0:00:42s\n",
      "epoch 9  | loss: 0.05495 | train_logloss: 0.21408 | val_logloss: 0.22749 |  0:00:47s\n",
      "epoch 10 | loss: 0.05481 | train_logloss: 0.21571 | val_logloss: 0.22436 |  0:00:52s\n",
      "epoch 11 | loss: 0.05478 | train_logloss: 0.22146 | val_logloss: 0.2475  |  0:00:57s\n",
      "epoch 12 | loss: 0.0547  | train_logloss: 0.21679 | val_logloss: 0.22246 |  0:01:02s\n",
      "epoch 13 | loss: 0.05472 | train_logloss: 0.22317 | val_logloss: 0.25142 |  0:01:06s\n",
      "epoch 14 | loss: 0.05442 | train_logloss: 0.21641 | val_logloss: 0.23171 |  0:01:11s\n",
      "epoch 15 | loss: 0.05429 | train_logloss: 0.21319 | val_logloss: 0.22295 |  0:01:16s\n",
      "epoch 16 | loss: 0.05427 | train_logloss: 0.21034 | val_logloss: 0.225   |  0:01:20s\n",
      "epoch 17 | loss: 0.054   | train_logloss: 0.20871 | val_logloss: 0.22649 |  0:01:25s\n",
      "epoch 18 | loss: 0.0536  | train_logloss: 0.20991 | val_logloss: 0.22416 |  0:01:30s\n",
      "epoch 19 | loss: 0.05365 | train_logloss: 0.20395 | val_logloss: 0.21452 |  0:01:35s\n",
      "Stop training because you reached max_epochs = 20 with best_epoch = 19 and best_val_logloss = 0.21452\n",
      "Best weights from best epoch are automatically used!\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.20624\n",
      "for year 19\n",
      "Device used : cpu\n",
      "Running the TabNet DNN, this could take a while\n",
      "epoch 0  | loss: 0.08586 | train_logloss: 0.22552 | val_logloss: 0.23135 |  0:00:10s\n",
      "epoch 1  | loss: 0.0565  | train_logloss: 0.21961 | val_logloss: 0.22615 |  0:00:20s\n",
      "epoch 2  | loss: 0.05611 | train_logloss: 0.21918 | val_logloss: 0.22645 |  0:00:31s\n",
      "epoch 3  | loss: 0.0558  | train_logloss: 0.21873 | val_logloss: 0.22505 |  0:00:41s\n",
      "epoch 4  | loss: 0.05532 | train_logloss: 0.21271 | val_logloss: 0.21968 |  0:00:52s\n",
      "epoch 5  | loss: 0.05485 | train_logloss: 0.21129 | val_logloss: 0.22307 |  0:01:02s\n",
      "epoch 6  | loss: 0.05461 | train_logloss: 0.20978 | val_logloss: 0.21594 |  0:01:12s\n",
      "epoch 7  | loss: 0.05446 | train_logloss: 0.21197 | val_logloss: 0.22118 |  0:01:23s\n",
      "epoch 8  | loss: 0.05441 | train_logloss: 0.2141  | val_logloss: 0.22241 |  0:01:33s\n",
      "epoch 9  | loss: 0.05447 | train_logloss: 0.21395 | val_logloss: 0.22122 |  0:01:43s\n",
      "epoch 10 | loss: 0.05436 | train_logloss: 0.21029 | val_logloss: 0.21789 |  0:01:54s\n",
      "epoch 11 | loss: 0.0543  | train_logloss: 0.20855 | val_logloss: 0.21551 |  0:02:04s\n",
      "epoch 12 | loss: 0.05414 | train_logloss: 0.22731 | val_logloss: 0.23675 |  0:02:15s\n",
      "epoch 13 | loss: 0.05413 | train_logloss: 0.21173 | val_logloss: 0.22031 |  0:02:25s\n",
      "epoch 14 | loss: 0.05414 | train_logloss: 0.21091 | val_logloss: 0.21722 |  0:02:35s\n",
      "epoch 15 | loss: 0.05401 | train_logloss: 0.21026 | val_logloss: 0.22375 |  0:02:46s\n",
      "epoch 16 | loss: 0.05388 | train_logloss: 0.20889 | val_logloss: 0.22552 |  0:02:56s\n",
      "epoch 17 | loss: 0.05381 | train_logloss: 0.21397 | val_logloss: 0.22543 |  0:03:06s\n",
      "epoch 18 | loss: 0.05383 | train_logloss: 0.21475 | val_logloss: 0.2268  |  0:03:17s\n",
      "epoch 19 | loss: 0.05376 | train_logloss: 0.21556 | val_logloss: 0.23226 |  0:03:27s\n",
      "Stop training because you reached max_epochs = 20 with best_epoch = 11 and best_val_logloss = 0.21551\n",
      "Best weights from best epoch are automatically used!\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.20379\n",
      "for year 18\n",
      "Device used : cpu\n",
      "Running the TabNet DNN, this could take a while\n",
      "epoch 0  | loss: 0.07524 | train_logloss: 0.21935 | val_logloss: 0.21721 |  0:00:15s\n",
      "epoch 1  | loss: 0.05486 | train_logloss: 0.21181 | val_logloss: 0.20525 |  0:00:31s\n",
      "epoch 2  | loss: 0.05417 | train_logloss: 0.2142  | val_logloss: 0.20853 |  0:00:47s\n",
      "epoch 3  | loss: 0.05378 | train_logloss: 0.20746 | val_logloss: 0.2022  |  0:01:03s\n",
      "epoch 4  | loss: 0.05359 | train_logloss: 0.21007 | val_logloss: 0.20536 |  0:01:18s\n",
      "epoch 5  | loss: 0.05351 | train_logloss: 0.20788 | val_logloss: 0.20251 |  0:01:34s\n",
      "epoch 6  | loss: 0.05338 | train_logloss: 0.20495 | val_logloss: 0.20355 |  0:01:50s\n",
      "epoch 7  | loss: 0.05315 | train_logloss: 0.20697 | val_logloss: 0.20405 |  0:02:05s\n",
      "epoch 8  | loss: 0.0531  | train_logloss: 0.20543 | val_logloss: 0.20498 |  0:02:21s\n",
      "epoch 9  | loss: 0.05297 | train_logloss: 0.20567 | val_logloss: 0.20536 |  0:02:37s\n",
      "epoch 10 | loss: 0.05288 | train_logloss: 0.20414 | val_logloss: 0.20102 |  0:02:52s\n",
      "epoch 11 | loss: 0.05283 | train_logloss: 0.20338 | val_logloss: 0.20109 |  0:03:08s\n",
      "epoch 12 | loss: 0.05279 | train_logloss: 0.20352 | val_logloss: 0.20344 |  0:03:24s\n",
      "epoch 13 | loss: 0.05272 | train_logloss: 0.2021  | val_logloss: 0.19815 |  0:03:39s\n",
      "epoch 14 | loss: 0.05275 | train_logloss: 0.20174 | val_logloss: 0.1973  |  0:03:55s\n",
      "epoch 15 | loss: 0.05269 | train_logloss: 0.20186 | val_logloss: 0.19733 |  0:04:11s\n",
      "epoch 16 | loss: 0.05266 | train_logloss: 0.20129 | val_logloss: 0.19747 |  0:04:27s\n",
      "epoch 17 | loss: 0.05272 | train_logloss: 0.20303 | val_logloss: 0.19978 |  0:04:42s\n",
      "epoch 18 | loss: 0.05265 | train_logloss: 0.20252 | val_logloss: 0.19782 |  0:04:58s\n",
      "epoch 19 | loss: 0.05262 | train_logloss: 0.20317 | val_logloss: 0.19912 |  0:05:14s\n",
      "Stop training because you reached max_epochs = 20 with best_epoch = 14 and best_val_logloss = 0.1973\n",
      "Best weights from best epoch are automatically used!\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.20193\n",
      "for year 17\n",
      "Device used : cpu\n",
      "Running the TabNet DNN, this could take a while\n",
      "epoch 0  | loss: 0.0704  | train_logloss: 0.22131 | val_logloss: 0.21676 |  0:00:21s\n",
      "epoch 1  | loss: 0.05508 | train_logloss: 0.21796 | val_logloss: 0.21244 |  0:00:41s\n",
      "epoch 2  | loss: 0.05423 | train_logloss: 0.23537 | val_logloss: 0.22515 |  0:01:02s\n",
      "epoch 3  | loss: 0.05346 | train_logloss: 0.2067  | val_logloss: 0.20222 |  0:01:23s\n",
      "epoch 4  | loss: 0.0531  | train_logloss: 0.20769 | val_logloss: 0.20369 |  0:01:44s\n",
      "epoch 5  | loss: 0.05308 | train_logloss: 0.20339 | val_logloss: 0.19863 |  0:02:05s\n",
      "epoch 6  | loss: 0.05274 | train_logloss: 0.20545 | val_logloss: 0.20112 |  0:02:26s\n",
      "epoch 7  | loss: 0.05276 | train_logloss: 0.20288 | val_logloss: 0.19932 |  0:02:46s\n",
      "epoch 8  | loss: 0.05246 | train_logloss: 0.20196 | val_logloss: 0.19773 |  0:03:07s\n",
      "epoch 9  | loss: 0.05234 | train_logloss: 0.20081 | val_logloss: 0.1962  |  0:03:28s\n",
      "epoch 10 | loss: 0.05232 | train_logloss: 0.19981 | val_logloss: 0.19567 |  0:03:48s\n",
      "epoch 11 | loss: 0.05233 | train_logloss: 0.19971 | val_logloss: 0.19574 |  0:04:09s\n",
      "epoch 12 | loss: 0.05241 | train_logloss: 0.20459 | val_logloss: 0.19979 |  0:04:30s\n",
      "epoch 13 | loss: 0.05209 | train_logloss: 0.19923 | val_logloss: 0.19477 |  0:04:50s\n",
      "epoch 14 | loss: 0.05198 | train_logloss: 0.20021 | val_logloss: 0.19589 |  0:05:11s\n",
      "epoch 15 | loss: 0.05237 | train_logloss: 0.19952 | val_logloss: 0.19528 |  0:05:31s\n",
      "epoch 16 | loss: 0.05213 | train_logloss: 0.20005 | val_logloss: 0.19612 |  0:05:52s\n",
      "epoch 17 | loss: 0.05209 | train_logloss: 0.19958 | val_logloss: 0.19511 |  0:06:13s\n",
      "epoch 18 | loss: 0.05203 | train_logloss: 0.19963 | val_logloss: 0.19493 |  0:06:34s\n",
      "epoch 19 | loss: 0.05214 | train_logloss: 0.19908 | val_logloss: 0.19484 |  0:06:55s\n",
      "Stop training because you reached max_epochs = 20 with best_epoch = 13 and best_val_logloss = 0.19477\n",
      "Best weights from best epoch are automatically used!\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.20224\n",
      "for year 16\n",
      "Device used : cpu\n",
      "Running the TabNet DNN, this could take a while\n",
      "epoch 0  | loss: 0.06813 | train_logloss: 0.21982 | val_logloss: 0.2164  |  0:00:26s\n",
      "epoch 1  | loss: 0.05376 | train_logloss: 0.21416 | val_logloss: 0.20686 |  0:00:51s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2  | loss: 0.05322 | train_logloss: 0.20896 | val_logloss: 0.2031  |  0:01:17s\n",
      "epoch 3  | loss: 0.05284 | train_logloss: 0.20732 | val_logloss: 0.20172 |  0:01:43s\n",
      "epoch 4  | loss: 0.05255 | train_logloss: 0.21472 | val_logloss: 0.21233 |  0:02:09s\n",
      "epoch 5  | loss: 0.0524  | train_logloss: 0.20429 | val_logloss: 0.19818 |  0:02:35s\n",
      "epoch 6  | loss: 0.05221 | train_logloss: 0.20434 | val_logloss: 0.19876 |  0:03:00s\n",
      "epoch 7  | loss: 0.05209 | train_logloss: 0.2023  | val_logloss: 0.1994  |  0:03:23s\n",
      "epoch 8  | loss: 0.05218 | train_logloss: 0.20761 | val_logloss: 0.20024 |  0:03:46s\n",
      "epoch 9  | loss: 0.05203 | train_logloss: 0.20162 | val_logloss: 0.19855 |  0:04:09s\n",
      "epoch 10 | loss: 0.05194 | train_logloss: 0.20224 | val_logloss: 0.19933 |  0:04:32s\n",
      "epoch 11 | loss: 0.05188 | train_logloss: 0.20124 | val_logloss: 0.19635 |  0:04:56s\n",
      "epoch 12 | loss: 0.05175 | train_logloss: 0.2028  | val_logloss: 0.19839 |  0:05:19s\n",
      "epoch 13 | loss: 0.05166 | train_logloss: 0.20449 | val_logloss: 0.19938 |  0:05:42s\n",
      "epoch 14 | loss: 0.05158 | train_logloss: 0.20364 | val_logloss: 0.20242 |  0:06:05s\n",
      "epoch 15 | loss: 0.05154 | train_logloss: 0.20114 | val_logloss: 0.20147 |  0:06:28s\n",
      "epoch 16 | loss: 0.05156 | train_logloss: 0.21499 | val_logloss: 0.20718 |  0:06:51s\n",
      "epoch 17 | loss: 0.05147 | train_logloss: 0.20781 | val_logloss: 0.20514 |  0:07:15s\n",
      "epoch 18 | loss: 0.05149 | train_logloss: 0.2026  | val_logloss: 0.19696 |  0:07:38s\n",
      "epoch 19 | loss: 0.05142 | train_logloss: 0.22715 | val_logloss: 0.22134 |  0:08:01s\n",
      "Stop training because you reached max_epochs = 20 with best_epoch = 11 and best_val_logloss = 0.19635\n",
      "Best weights from best epoch are automatically used!\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19756\n",
      "for year 15\n",
      "Device used : cpu\n",
      "Running the TabNet DNN, this could take a while\n",
      "epoch 0  | loss: 0.06421 | train_logloss: 0.20894 | val_logloss: 0.20853 |  0:00:27s\n",
      "epoch 1  | loss: 0.05316 | train_logloss: 0.20694 | val_logloss: 0.20584 |  0:00:55s\n",
      "epoch 2  | loss: 0.05265 | train_logloss: 0.20737 | val_logloss: 0.21061 |  0:01:23s\n",
      "epoch 3  | loss: 0.05224 | train_logloss: 0.20176 | val_logloss: 0.20313 |  0:01:51s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wolgy\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2279: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4  | loss: 0.05186 | train_logloss: inf     | val_logloss: 0.19878 |  0:02:18s\n",
      "epoch 5  | loss: 0.05163 | train_logloss: 0.20185 | val_logloss: 0.2032  |  0:02:46s\n",
      "epoch 6  | loss: 0.05148 | train_logloss: 0.19858 | val_logloss: 0.19787 |  0:03:13s\n",
      "epoch 7  | loss: 0.05137 | train_logloss: 0.20044 | val_logloss: 0.20057 |  0:03:41s\n",
      "epoch 8  | loss: 0.0513  | train_logloss: 0.19974 | val_logloss: 0.19907 |  0:04:08s\n",
      "epoch 9  | loss: 0.0512  | train_logloss: 0.19624 | val_logloss: 0.19606 |  0:04:36s\n",
      "epoch 10 | loss: 0.05108 | train_logloss: 0.19924 | val_logloss: 0.1979  |  0:05:04s\n",
      "epoch 11 | loss: 0.051   | train_logloss: 0.19662 | val_logloss: 0.19634 |  0:05:31s\n",
      "epoch 12 | loss: 0.05098 | train_logloss: 0.19559 | val_logloss: 0.19589 |  0:05:59s\n",
      "epoch 13 | loss: 0.05094 | train_logloss: 0.19569 | val_logloss: 0.19532 |  0:06:26s\n",
      "epoch 14 | loss: 0.0509  | train_logloss: 0.19725 | val_logloss: 0.19798 |  0:06:54s\n",
      "epoch 15 | loss: 0.05082 | train_logloss: 0.19466 | val_logloss: 0.19431 |  0:07:22s\n",
      "epoch 16 | loss: 0.05075 | train_logloss: 0.19507 | val_logloss: 0.1952  |  0:07:49s\n",
      "epoch 17 | loss: 0.05072 | train_logloss: 0.19575 | val_logloss: 0.19508 |  0:08:17s\n",
      "epoch 18 | loss: 0.05072 | train_logloss: 0.19326 | val_logloss: 0.19344 |  0:08:44s\n",
      "epoch 19 | loss: 0.05063 | train_logloss: 0.19277 | val_logloss: 0.19267 |  0:09:12s\n",
      "Stop training because you reached max_epochs = 20 with best_epoch = 19 and best_val_logloss = 0.19267\n",
      "Best weights from best epoch are automatically used!\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19529\n",
      "for year 14\n",
      "Device used : cpu\n",
      "Running the TabNet DNN, this could take a while\n",
      "epoch 0  | loss: 0.06322 | train_logloss: 0.21829 | val_logloss: 0.22089 |  0:00:32s\n",
      "epoch 1  | loss: 0.05323 | train_logloss: 0.2063  | val_logloss: 0.20526 |  0:01:04s\n",
      "epoch 2  | loss: 0.05212 | train_logloss: 0.19934 | val_logloss: 0.19825 |  0:01:36s\n",
      "epoch 3  | loss: 0.0516  | train_logloss: 0.19879 | val_logloss: 0.19743 |  0:02:09s\n",
      "epoch 4  | loss: 0.05143 | train_logloss: 0.19772 | val_logloss: 0.19586 |  0:02:41s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wolgy\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2279: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\Wolgy\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2279: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\Wolgy\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2279: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5  | loss: 0.05131 | train_logloss: nan     | val_logloss: inf     |  0:03:13s\n",
      "epoch 6  | loss: 0.05121 | train_logloss: 0.19766 | val_logloss: 0.19583 |  0:03:45s\n",
      "epoch 7  | loss: 0.0511  | train_logloss: 0.19615 | val_logloss: 0.1956  |  0:04:17s\n",
      "epoch 8  | loss: 0.05094 | train_logloss: 0.19458 | val_logloss: 0.1925  |  0:04:49s\n",
      "epoch 9  | loss: 0.05089 | train_logloss: 0.19392 | val_logloss: 0.19245 |  0:05:20s\n",
      "epoch 10 | loss: 0.05072 | train_logloss: 0.19314 | val_logloss: 0.19138 |  0:05:52s\n",
      "epoch 11 | loss: 0.05061 | train_logloss: 0.19275 | val_logloss: 0.19097 |  0:06:24s\n",
      "epoch 12 | loss: 0.05057 | train_logloss: 0.19336 | val_logloss: 0.19148 |  0:06:56s\n",
      "epoch 13 | loss: 0.05054 | train_logloss: 0.19286 | val_logloss: 0.19129 |  0:07:28s\n",
      "epoch 14 | loss: 0.0505  | train_logloss: 0.19443 | val_logloss: 0.19258 |  0:07:59s\n",
      "epoch 15 | loss: 0.0505  | train_logloss: 0.19318 | val_logloss: 0.19166 |  0:08:31s\n",
      "epoch 16 | loss: 0.05049 | train_logloss: 0.19398 | val_logloss: 0.1923  |  0:09:03s\n",
      "epoch 17 | loss: 0.05047 | train_logloss: 0.19256 | val_logloss: 0.19081 |  0:09:34s\n",
      "epoch 18 | loss: 0.05038 | train_logloss: 0.19332 | val_logloss: 0.19164 |  0:10:06s\n",
      "epoch 19 | loss: 0.0504  | train_logloss: 0.19291 | val_logloss: 0.19177 |  0:10:38s\n",
      "Stop training because you reached max_epochs = 20 with best_epoch = 17 and best_val_logloss = 0.19081\n",
      "Best weights from best epoch are automatically used!\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19347\n",
      "for year 13\n",
      "Device used : cpu\n",
      "Running the TabNet DNN, this could take a while\n",
      "epoch 0  | loss: 0.06351 | train_logloss: 0.21149 | val_logloss: 0.21153 |  0:00:35s\n",
      "epoch 1  | loss: 0.05261 | train_logloss: 0.20678 | val_logloss: 0.20605 |  0:01:09s\n",
      "epoch 2  | loss: 0.05216 | train_logloss: 0.22582 | val_logloss: 0.22635 |  0:01:44s\n",
      "epoch 3  | loss: 0.05168 | train_logloss: 0.21315 | val_logloss: 0.21248 |  0:02:18s\n",
      "epoch 4  | loss: 0.05148 | train_logloss: 0.19807 | val_logloss: 0.19894 |  0:02:53s\n",
      "epoch 5  | loss: 0.05122 | train_logloss: 0.19941 | val_logloss: 0.20067 |  0:03:27s\n",
      "epoch 6  | loss: 0.05107 | train_logloss: 0.21794 | val_logloss: 0.21786 |  0:04:02s\n",
      "epoch 7  | loss: 0.05095 | train_logloss: 0.2027  | val_logloss: 0.20591 |  0:04:36s\n",
      "epoch 8  | loss: 0.0509  | train_logloss: 0.20856 | val_logloss: 0.20959 |  0:05:10s\n",
      "epoch 9  | loss: 0.05078 | train_logloss: 0.1998  | val_logloss: 0.20289 |  0:05:45s\n",
      "epoch 10 | loss: 0.05065 | train_logloss: 0.19573 | val_logloss: 0.19804 |  0:06:19s\n",
      "epoch 11 | loss: 0.05053 | train_logloss: 0.19271 | val_logloss: 0.19381 |  0:06:54s\n",
      "epoch 12 | loss: 0.05047 | train_logloss: 0.1947  | val_logloss: 0.1958  |  0:07:28s\n",
      "epoch 13 | loss: 0.05048 | train_logloss: 0.19651 | val_logloss: 0.19714 |  0:08:03s\n",
      "epoch 14 | loss: 0.05039 | train_logloss: 0.1968  | val_logloss: 0.20043 |  0:08:37s\n",
      "epoch 15 | loss: 0.05038 | train_logloss: 0.19242 | val_logloss: 0.19355 |  0:09:11s\n",
      "epoch 16 | loss: 0.05034 | train_logloss: 0.19155 | val_logloss: 0.19252 |  0:09:46s\n",
      "epoch 17 | loss: 0.05035 | train_logloss: 0.1939  | val_logloss: 0.195   |  0:10:20s\n",
      "epoch 18 | loss: 0.05038 | train_logloss: 0.19434 | val_logloss: 0.19503 |  0:10:54s\n",
      "epoch 19 | loss: 0.05051 | train_logloss: 0.19577 | val_logloss: 0.1959  |  0:11:29s\n",
      "Stop training because you reached max_epochs = 20 with best_epoch = 16 and best_val_logloss = 0.19252\n",
      "Best weights from best epoch are automatically used!\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.1945\n",
      "for year 12\n",
      "Device used : cpu\n",
      "Running the TabNet DNN, this could take a while\n",
      "epoch 0  | loss: 0.06134 | train_logloss: 0.21506 | val_logloss: 0.22067 |  0:00:39s\n",
      "epoch 1  | loss: 0.05285 | train_logloss: 0.20421 | val_logloss: 0.2086  |  0:01:18s\n",
      "epoch 2  | loss: 0.05147 | train_logloss: 0.20363 | val_logloss: 0.20845 |  0:01:57s\n",
      "epoch 3  | loss: 0.05095 | train_logloss: 0.19607 | val_logloss: 0.19991 |  0:02:35s\n",
      "epoch 4  | loss: 0.0507  | train_logloss: 0.19577 | val_logloss: 0.20009 |  0:03:14s\n",
      "epoch 5  | loss: 0.05054 | train_logloss: 0.19448 | val_logloss: 0.19866 |  0:03:53s\n",
      "epoch 6  | loss: 0.05034 | train_logloss: 0.21012 | val_logloss: 0.21581 |  0:04:32s\n",
      "epoch 7  | loss: 0.05024 | train_logloss: 0.19271 | val_logloss: 0.19686 |  0:05:11s\n",
      "epoch 8  | loss: 0.05028 | train_logloss: 0.19428 | val_logloss: 0.1994  |  0:05:50s\n",
      "epoch 9  | loss: 0.0502  | train_logloss: 0.19438 | val_logloss: 0.19901 |  0:06:29s\n",
      "epoch 10 | loss: 0.05014 | train_logloss: 0.19294 | val_logloss: 0.19672 |  0:07:07s\n",
      "epoch 11 | loss: 0.05011 | train_logloss: 0.19158 | val_logloss: 0.19594 |  0:07:46s\n",
      "epoch 12 | loss: 0.05007 | train_logloss: 0.19289 | val_logloss: 0.19702 |  0:08:25s\n",
      "epoch 13 | loss: 0.05005 | train_logloss: 0.19137 | val_logloss: 0.19537 |  0:09:04s\n",
      "epoch 14 | loss: 0.05001 | train_logloss: 0.19242 | val_logloss: 0.19683 |  0:09:43s\n",
      "epoch 15 | loss: 0.04999 | train_logloss: 0.19167 | val_logloss: 0.19627 |  0:10:21s\n",
      "epoch 16 | loss: 0.04997 | train_logloss: 0.19124 | val_logloss: 0.19564 |  0:11:00s\n",
      "epoch 17 | loss: 0.04993 | train_logloss: 0.19429 | val_logloss: 0.19865 |  0:11:39s\n",
      "epoch 18 | loss: 0.04993 | train_logloss: 0.19687 | val_logloss: 0.20119 |  0:12:17s\n",
      "epoch 19 | loss: 0.04988 | train_logloss: 0.19392 | val_logloss: 0.19867 |  0:12:56s\n",
      "Stop training because you reached max_epochs = 20 with best_epoch = 13 and best_val_logloss = 0.19537\n",
      "Best weights from best epoch are automatically used!\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19442\n",
      "for year 11\n",
      "Device used : cpu\n",
      "Running the TabNet DNN, this could take a while\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wolgy\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2279: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.06065 | train_logloss: inf     | val_logloss: 0.20517 |  0:00:43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wolgy\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2279: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1  | loss: 0.05165 | train_logloss: inf     | val_logloss: 0.20215 |  0:01:27s\n",
      "epoch 2  | loss: 0.0514  | train_logloss: 0.19865 | val_logloss: 0.19826 |  0:02:10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wolgy\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2279: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.05113 | train_logloss: inf     | val_logloss: 0.19891 |  0:02:54s\n",
      "epoch 4  | loss: 0.05101 | train_logloss: 0.19764 | val_logloss: 0.1978  |  0:03:37s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wolgy\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2279: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5  | loss: 0.05097 | train_logloss: inf     | val_logloss: 0.20025 |  0:04:21s\n",
      "epoch 6  | loss: 0.05081 | train_logloss: 0.19805 | val_logloss: 0.19863 |  0:05:04s\n",
      "epoch 7  | loss: 0.05072 | train_logloss: 0.19977 | val_logloss: 0.19853 |  0:05:48s\n",
      "epoch 8  | loss: 0.05062 | train_logloss: 0.19577 | val_logloss: 0.19598 |  0:06:32s\n",
      "epoch 9  | loss: 0.05068 | train_logloss: 0.19396 | val_logloss: 0.19368 |  0:07:16s\n",
      "epoch 10 | loss: 0.05057 | train_logloss: 0.19527 | val_logloss: 0.19511 |  0:07:59s\n",
      "epoch 11 | loss: 0.05107 | train_logloss: 0.19842 | val_logloss: 0.19782 |  0:08:43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wolgy\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2279: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 | loss: 0.05099 | train_logloss: inf     | val_logloss: 0.20072 |  0:09:26s\n",
      "epoch 13 | loss: 0.05094 | train_logloss: 0.19644 | val_logloss: 0.19638 |  0:10:09s\n",
      "epoch 14 | loss: 0.05097 | train_logloss: 0.19836 | val_logloss: 0.19804 |  0:10:53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wolgy\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2279: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 | loss: 0.05095 | train_logloss: inf     | val_logloss: 0.19769 |  0:11:36s\n",
      "epoch 16 | loss: 0.05098 | train_logloss: 0.19789 | val_logloss: 0.19685 |  0:12:19s\n",
      "epoch 17 | loss: 0.05089 | train_logloss: 0.2111  | val_logloss: 0.20918 |  0:13:02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wolgy\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2279: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 | loss: 0.05082 | train_logloss: inf     | val_logloss: 0.20228 |  0:13:46s\n",
      "epoch 19 | loss: 0.05077 | train_logloss: 0.20457 | val_logloss: 0.20459 |  0:14:29s\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_logloss = 0.19368\n",
      "Best weights from best epoch are automatically used!\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19481\n"
     ]
    }
   ],
   "source": [
    "df_2020 = pd.concat([df_20])\n",
    "df_1920 = pd.concat([df_20,df_19])\n",
    "df_1820 = pd.concat([df_20,df_19,df_18])\n",
    "df_1720 = pd.concat([df_20,df_19,df_18,df_17])\n",
    "df_1620 = pd.concat([df_20,df_19,df_18,df_17,df_16])\n",
    "df_1520 = pd.concat([df_20,df_19,df_18,df_17,df_16,df_15])\n",
    "df_1420 = pd.concat([df_20,df_19,df_18,df_17,df_16,df_15,df_14])\n",
    "df_1320 = pd.concat([df_20,df_19,df_18,df_17,df_16,df_15,df_14,df_13])\n",
    "df_1220 = pd.concat([df_20,df_19,df_18,df_17,df_16,df_15,df_14,df_13,df_12])\n",
    "df_1120 = pd.concat([df_20,df_19,df_18,df_17,df_16,df_15,df_14,df_13,df_12,df_11])\n",
    "lst = [df_2020,df_1920,df_1820,df_1720,df_1620,df_1520,df_1420,df_1320,df_1220,df_1120]\n",
    "start_year = 20\n",
    "for df in lst:\n",
    "    print(f'for year {start_year}')\n",
    "    mm.run_tabnet(df)    \n",
    "    start_year -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0983f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for year 20\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19716\n",
      "for year 19\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.20104\n",
      "for year 18\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19614\n",
      "for year 17\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19226\n",
      "for year 16\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19075\n",
      "for year 15\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19098\n",
      "for year 14\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19031\n",
      "for year 13\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19\n",
      "for year 12\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.19134\n",
      "for year 11\n",
      "Running XGB model, please be patient\n",
      "Fitting the test data to the model\n",
      "The Log loss is 0.18771\n"
     ]
    }
   ],
   "source": [
    "df_2020 = pd.concat([df_20])\n",
    "df_1920 = pd.concat([df_20,df_19])\n",
    "df_1820 = pd.concat([df_20,df_19,df_18])\n",
    "df_1720 = pd.concat([df_20,df_19,df_18,df_17])\n",
    "df_1620 = pd.concat([df_20,df_19,df_18,df_17,df_16])\n",
    "df_1520 = pd.concat([df_20,df_19,df_18,df_17,df_16,df_15])\n",
    "df_1420 = pd.concat([df_20,df_19,df_18,df_17,df_16,df_15,df_14])\n",
    "df_1320 = pd.concat([df_20,df_19,df_18,df_17,df_16,df_15,df_14,df_13])\n",
    "df_1220 = pd.concat([df_20,df_19,df_18,df_17,df_16,df_15,df_14,df_13,df_12])\n",
    "df_1120 = pd.concat([df_20,df_19,df_18,df_17,df_16,df_15,df_14,df_13,df_12,df_11])\n",
    "lst = [df_2020,df_1920,df_1820,df_1720,df_1620,df_1520,df_1420,df_1320,df_1220,df_1120]\n",
    "start_year = 20\n",
    "for df in lst:\n",
    "    print(f'for year {start_year}')\n",
    "    mm.run_XGB(df,random_state=42)    \n",
    "    start_year -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e781b01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
